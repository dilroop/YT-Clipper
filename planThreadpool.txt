# ThreadPoolExecutor Quick Fix - Server Blocking Issue

## Problem Overview

**Current Issue:**
The FastAPI server becomes completely unresponsive during video processing because blocking I/O operations (FFmpeg subprocess.run(), MediaPipe face detection, OpenCV) are running synchronously in async endpoints, blocking the entire event loop.

**Symptoms:**
- Server hangs during video processing
- Cannot handle new HTTP requests while processing
- WebSocket connections timeout
- UI becomes unresponsive

**Root Cause:**
```python
# In backend/server.py lines 525-894
@app.post("/api/process")
async def process_video(request: ProcessVideoRequest):  # ‚ùå async def with blocking operations
    # These are ALL blocking operations:
    download_result = downloader.download_video(request.url, update_progress_sync)  # subprocess.run()
    transcript_result = transcriber.transcribe(video_path, update_progress_sync)    # Whisper model inference
    clipper.create_clip(...)                                                        # subprocess.run() for FFmpeg
    caption_gen.burn_captions(...)                                                 # subprocess.run() for FFmpeg
    reels_proc.convert_to_reels(...)                                               # subprocess.run() + MediaPipe
    watermark_proc.add_watermark(...)                                              # subprocess.run()
```

## Solution: ThreadPoolExecutor (Quick Fix)

**Approach:**
Move all CPU-bound and blocking I/O operations to a thread pool, allowing the event loop to continue handling other requests.

**Estimated Time:** 1-2 hours
**Dependencies:** None (Python stdlib)
**Complexity:** Low

---

## Implementation Steps

### Step 1: Create ThreadPoolExecutor Instance

**File:** `backend/server.py`
**Location:** After imports, before app initialization (around line 41)

```python
import concurrent.futures
from functools import partial

# Create thread pool for blocking operations
# Max workers = 2 to prevent overloading CPU with multiple FFmpeg processes
executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
```

**Why 2 workers?**
- FFmpeg and MediaPipe are CPU-intensive
- Running 3+ simultaneous video processes can overwhelm system
- 2 allows one active + one queued without resource exhaustion

---

### Step 2: Create Async Wrapper Function

**File:** `backend/server.py`
**Location:** After executor creation (around line 45)

```python
async def run_in_executor(func, *args, **kwargs):
    """
    Run blocking function in thread pool executor

    Args:
        func: Blocking function to run
        *args: Positional arguments
        **kwargs: Keyword arguments

    Returns:
        Result from blocking function
    """
    loop = asyncio.get_event_loop()
    if kwargs:
        func = partial(func, **kwargs)
    return await loop.run_in_executor(executor, func, *args)
```

---

### Step 3: Wrap Blocking Operations

**File:** `backend/server.py`

#### 3a. Wrap video download (line 614)

**BEFORE:**
```python
download_result = downloader.download_video(request.url, update_progress_sync)
```

**AFTER:**
```python
download_result = await run_in_executor(
    downloader.download_video,
    request.url,
    update_progress_sync
)
```

#### 3b. Wrap transcription (line 662)

**BEFORE:**
```python
transcript_result = transcriber.transcribe(video_path, update_progress_sync)
```

**AFTER:**
```python
transcript_result = await run_in_executor(
    transcriber.transcribe,
    video_path,
    update_progress_sync
)
```

#### 3c. Wrap clip creation (lines 735-745)

**BEFORE:**
```python
if 'parts' in clip and len(clip['parts']) > 1:
    clip_result = clipper.create_multipart_clip(
        video_path=video_path,
        parts=clip['parts']
    )
else:
    clip_result = clipper.create_clip(
        video_path=video_path,
        start_time=clip['start'],
        end_time=clip['end']
    )
```

**AFTER:**
```python
if 'parts' in clip and len(clip['parts']) > 1:
    clip_result = await run_in_executor(
        clipper.create_multipart_clip,
        video_path=str(video_path),
        parts=clip['parts']
    )
else:
    clip_result = await run_in_executor(
        clipper.create_clip,
        video_path=str(video_path),
        start_time=clip['start'],
        end_time=clip['end']
    )
```

#### 3d. Wrap reels conversion (line 785)

**BEFORE:**
```python
reels_result = reels_proc.convert_to_reels(
    clip_path,
    output_format=output_format_map.get(request.format, "vertical_9x16")
)
```

**AFTER:**
```python
reels_result = await run_in_executor(
    reels_proc.convert_to_reels,
    clip_path,
    output_format=output_format_map.get(request.format, "vertical_9x16")
)
```

#### 3e. Wrap caption burning (line 813)

**BEFORE:**
```python
burn_result = caption_gen.burn_captions(
    video_path=clip_path,
    subtitle_path=str(ass_path),
    output_path=str(captioned_path)
)
```

**AFTER:**
```python
burn_result = await run_in_executor(
    caption_gen.burn_captions,
    video_path=clip_path,
    subtitle_path=str(ass_path),
    output_path=str(captioned_path)
)
```

#### 3f. Wrap watermark addition (line 824)

**BEFORE:**
```python
watermark_result = watermark_proc.add_watermark(clip_path)
```

**AFTER:**
```python
watermark_result = await run_in_executor(
    watermark_proc.add_watermark,
    clip_path
)
```

---

### Step 4: Apply Same Fix to /api/analyze Endpoint

**File:** `backend/server.py`
**Location:** Lines 343-522 (analyze_video function)

Wrap the same operations:

```python
# Line 418 - Download
download_result = await run_in_executor(
    downloader.download_video,
    request.url,
    update_progress_sync
)

# Line 434 - Transcribe
transcript_result = await run_in_executor(
    transcriber.transcribe,
    video_path,
    update_progress_sync
)
```

**Note:** AI analysis (GPT API call) is already async via OpenAI client, no change needed.

---

### Step 5: Graceful Shutdown

**File:** `backend/server.py`
**Location:** At the end of the file, before `if __name__ == "__main__":` (around line 1220)

```python
@app.on_event("shutdown")
async def shutdown_event():
    """Gracefully shutdown thread pool executor"""
    print("üõë Shutting down thread pool executor...")
    executor.shutdown(wait=True, cancel_futures=False)
    print("‚úÖ Thread pool executor shut down")
```

---

## Testing Procedure

### Test 1: Server Responsiveness During Processing

1. Start processing a video (long video, 10+ minutes recommended)
2. **While processing is ongoing**, open a new browser tab
3. Try to access `http://localhost:5000/api/health`
4. **Expected:** Health check responds immediately (< 100ms)
5. **Before fix:** Request hangs until video processing completes

### Test 2: Multiple Concurrent Requests

1. Open two browser tabs
2. Start processing Video A in Tab 1
3. Immediately start processing Video B in Tab 2
4. **Expected:**
   - First video processes normally
   - Second video queues and starts when first completes
   - Both tabs receive WebSocket progress updates
5. **Before fix:** Second request hangs entire server

### Test 3: WebSocket Stability

1. Start video processing with browser DevTools Network tab open
2. Monitor WebSocket connection status
3. **Expected:** Connection stays alive with heartbeat pings every 10s
4. **Before fix:** Connection times out after ~30s of blocking

### Test 4: Error Handling

1. Submit an invalid YouTube URL
2. **Expected:** Error returned immediately, doesn't hang
3. Submit a valid URL, then kill server mid-process
4. **Expected:** Cleanup runs, temp files deleted, no zombies

---

## Performance Considerations

### Memory Usage
- **ThreadPoolExecutor:** Minimal overhead (~1-2MB per worker)
- **Thread safety:** Python's GIL ensures thread-safe access to shared state

### Latency
- **No added latency** for operations (same subprocess.run() calls)
- **Queueing delay:** If 2+ requests arrive simultaneously, second waits in queue

### CPU Usage
- **Max 2 simultaneous video processes** prevents CPU overload
- FFmpeg already uses multiple cores efficiently

---

## Pros vs Cons: ThreadPoolExecutor vs Celery

| Feature | ThreadPoolExecutor (Quick Fix) | Celery + Redis (Production) |
|---------|-------------------------------|----------------------------|
| **Implementation Time** | 1-2 hours | 4-6 hours |
| **External Dependencies** | None | Redis server required |
| **Scalability** | Limited (2 workers) | Unlimited (multi-machine) |
| **Task Monitoring** | Basic (WebSocket only) | Advanced (Flower UI) |
| **Task Queue** | In-memory (lost on crash) | Persistent (Redis) |
| **Failure Recovery** | Manual restart | Auto-retry + DLQ |
| **Deployment Complexity** | None | Redis + worker process |
| **Best For** | Single-server, low traffic | Production, high traffic |

**Recommendation:**
- **Use ThreadPoolExecutor now** for immediate fix
- **Migrate to Celery later** if:
  - Processing >10 videos/hour
  - Need task scheduling
  - Require multi-server deployment
  - Want advanced monitoring

---

## Migration Path: ThreadPoolExecutor ‚Üí Celery

When ready to upgrade, follow these steps:

### 1. Install Dependencies
```bash
pip install celery redis flower
brew install redis  # macOS
brew services start redis
```

### 2. Create Celery App
**File:** `backend/celery_app.py` (new file)
```python
from celery import Celery

celery_app = Celery(
    'ytclipper',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)
```

### 3. Move Blocking Functions to Tasks
**File:** `backend/tasks.py` (new file)
```python
from celery_app import celery_app
from downloader import VideoDownloader
# ... other imports

@celery_app.task(bind=True)
def process_video_task(self, url, format_type, burn_captions, ...):
    # Move entire processing logic here
    pass
```

### 4. Update Server Endpoints
```python
@app.post("/api/process")
async def process_video(request: ProcessVideoRequest):
    # Dispatch task to Celery
    task = process_video_task.apply_async(args=[...])
    return {"task_id": task.id}

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    task = celery_app.AsyncResult(task_id)
    return {"state": task.state, "progress": task.info}
```

### 5. Start Worker Process
```bash
celery -A backend.celery_app worker --loglevel=info
```

### 6. Monitor with Flower
```bash
celery -A backend.celery_app flower
# Open http://localhost:5555
```

---

## Rollback Plan

If ThreadPoolExecutor causes issues:

1. **Revert code changes:**
   ```bash
   git diff HEAD backend/server.py > threadpool_changes.patch
   git checkout HEAD backend/server.py
   ```

2. **Remove executor creation:**
   - Delete `executor = ThreadPoolExecutor(...)` line
   - Delete `run_in_executor()` function

3. **Restore original blocking calls:**
   - Replace `await run_in_executor(func, ...)` with `func(...)`

4. **Restart server:**
   ```bash
   pkill -f "uvicorn backend.server:app"
   python backend/server.py
   ```

---

## Known Limitations

### 1. Progress Updates May Be Delayed
- Progress callbacks still run in worker threads
- WebSocket broadcasts might batch if event loop is busy
- **Mitigation:** Already handled via `asyncio.create_task()`

### 2. No Task Persistence
- If server crashes mid-process, task is lost
- User must retry from UI
- **Mitigation:** Migrate to Celery for production

### 3. Limited Concurrency
- Max 2 simultaneous video processes
- 3rd request waits in queue
- **Mitigation:** Increase `max_workers` if CPU allows, or use Celery

### 4. No Task Cancellation
- Once started, task runs to completion
- User can't cancel mid-process
- **Mitigation:** Add cancel endpoint with `executor.shutdown(cancel_futures=True)`

---

## Troubleshooting

### Issue: Server still hangs after applying fix

**Cause:** Missed wrapping a blocking operation

**Solution:**
1. Check server logs for which stage hangs
2. Verify all subprocess.run() calls are wrapped
3. Add debug prints before/after each `await run_in_executor()`

### Issue: "Event loop is closed" error

**Cause:** Executor trying to access closed loop during shutdown

**Solution:**
```python
@app.on_event("shutdown")
async def shutdown_event():
    executor.shutdown(wait=False)  # Don't wait, just signal shutdown
```

### Issue: Progress updates stop working

**Cause:** Callback functions losing event loop reference

**Solution:**
Ensure `update_progress_sync()` uses `asyncio.get_event_loop()` instead of storing loop reference.

---

## Summary

**What This Fix Does:**
‚úÖ Allows server to handle multiple requests during video processing
‚úÖ WebSocket connections stay alive with heartbeat
‚úÖ UI remains responsive
‚úÖ Health checks respond immediately
‚úÖ No external dependencies required

**What This Fix Doesn't Do:**
‚ùå Doesn't add task persistence (lost on crash)
‚ùå Doesn't scale beyond 2 concurrent processes
‚ùå Doesn't provide advanced monitoring
‚ùå Doesn't allow task cancellation

**Next Steps:**
1. Apply code changes from Step 1-5
2. Restart server
3. Run Test 1-4 to verify
4. Monitor logs for any issues
5. Plan Celery migration if needed

**Estimated Impact:**
- Implementation: 1-2 hours
- Testing: 30 minutes
- Risk: Low (easily reversible)
- Benefit: Server no longer blocks during processing

---

## Code Checklist

Before deploying, verify:

- [ ] ThreadPoolExecutor created with max_workers=2
- [ ] `run_in_executor()` wrapper function added
- [ ] All subprocess.run() calls wrapped in /api/process
- [ ] All subprocess.run() calls wrapped in /api/analyze
- [ ] Shutdown handler added for graceful cleanup
- [ ] Server restarts successfully
- [ ] Test 1 passes (health check during processing)
- [ ] Test 2 passes (concurrent requests)
- [ ] Test 3 passes (WebSocket stability)
- [ ] Test 4 passes (error handling)

**After all tests pass:** Consider this quick fix production-ready for single-server deployment.

**When to migrate to Celery:** If you need task persistence, advanced monitoring, or scaling beyond 2 workers.
